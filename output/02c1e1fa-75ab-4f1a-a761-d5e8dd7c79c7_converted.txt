 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
UNIT â€“ 1 
INTRODUCTION 
1. Some Representative Problems 
ïƒ˜ A First Problem: Stable Matching:ï€ 
ï‚§ The Problem 
ï‚§ Designing the Algorithm 
ï‚§ Analyzing the Algorithm 
ï‚§ Extensions 
2. Five Representative Problems: 
ïƒ˜ Interval Schedulingï€ 
ïƒ˜ Weighted Interval Schedulingï€ 
ïƒ˜ Bipartite Matchingï€ 
ïƒ˜ Independent Setï€ 
ïƒ˜ Competitive Facility Locationï€ 
3. Computational Tractability: 
ïƒ˜ Some Initial Attempts at Defining Efficiencyï€ 
ïƒ˜ Worst-Case Running Times and Brute-Force Searchï€ 
ïƒ˜ Polynomial Time as a Definition of Efficiencyï€ 
4. Asymptotic Order of Growth: 
ïƒ˜ Properties of Asymptotic Growth Ratesï€ 
ïƒ˜ Asymptotic Bounds for Some Common Functionsï€ 
5. Implementing the Stable Matching Algorithm 
ïƒ˜ Using Lists and Arrays: Arrays and Lists,ï€ 
ïƒ˜ Implementing the Stable Matching Algorithmï€ 
6. A Survey of Common Running Times: 
ïƒ˜ Linear Timeï€ 
ïƒ˜ O(n log n) Timeï€ 
ïƒ˜ Quadratic Timeï€ 
ïƒ˜ Cubic Timeï€ 
ïƒ˜ O(nk) Timeï€ 
ïƒ˜ Beyond Polynomial Timeï€ 
ïƒ˜ Sub linear Time.ï€ 
 
CHAPTER 1 
INTRODUCTION 
 
 
UNIT 1 
INTRODUCTION 
1. A first problem: Stable Matching 
1.1 
The problem: 
ï‚· Designing a college admission process or job recruiting process that is self-enforcing. 
 
ï‚· All juniors in college majoring in computer science begin applying to companies for summer 
internships. 
ï‚· Application process is the interplay between two different types of parties. 
1. Companies (the employers) 
2. Students (the applicants) 
ï‚· Each applicant has a preference ordering on companies and each company forms a 
preference ordering on its applicants. 
ï‚· Based on these preferences, companies extend offers to some of their applicants, applicants 
choose which of their offers to accept. 
ï‚· Gale and Shapely considered the sorts  of things  that could start going wrong with the 
process. 
1. â€Rajâ€ accepted job at company â€œCluNetâ€. 
2. â€WebExodusâ€ offers job to â€œRajâ€. 
3. â€Rajâ€ now prefers â€œWebExodusâ€ and rejects â€œCluNetâ€. 
4. â€Kiranâ€ gets an offer from â€œCluNetâ€. 
5. â€Kiranâ€ already had accepted offer from â€œBabelSoftâ€. 
6. â€Kiranâ€ accepts offer from â€œCluNetâ€ and rejects â€œBabelSoftâ€. 
7. â€Deepaâ€ who has accepted the offer from â€œBabelSoftâ€calls upâ€œWebExodusâ€ to join 
them (i.e. she preferred WebExodus over BabelSoft. 
8. â€WebExodusâ€ rejects â€œRajâ€ and accept â€œDeepaâ€ (i.e. WebExodus preferred Deepa 
over Raj). 
ï‚· Situation like this creates chaos and both applicants and employers endup unhappy with the 
process as well as outcome as the process is not self-enforcing and people are not allowed to 
act in their self-interest. 
 
 
CHAPTER 1 
INTRODUCTION 
 
 
ï‚· According to Gale and Shapley: Given a set of preferences among employers and applicants, 
we can assign applicants to employers so that for every employer E, and every applicant A 
who is not scheduled to work for E, at least one of the following two things should hold: 
1. â€œEâ€ prefers every one of its accepted applicants to â€œAâ€. 
2. â€œAâ€ prefers her current situation over working for employer â€œEâ€. 
If this holds, the outcome is stable. 
ï‚· 
 Individual self-interest will prevent any applicant/employer deal from being made 
behind the scene. 
 
1.2. Formulating the problem: 
ï‚· Each applicant is looking for a single company. Each company is looking for many 
applicants. Each applicant does not typically apply to every company. 
ï‚· Each of napplicants applies to each of ncompanies and each company wants to accept a 
single applicant. 
(OR) 
ï‚· nmen and n women can end up getting married, in this case everyone is seeking to be 
paired with exactly one individual of opposite gender. 
ïƒ˜ M is a set of n men, M={m1,m2,â€¦â€¦,mn} 
ïƒ˜ W is a set of n women, W={w1,w2,â€¦â€¦wn} 
ïƒ˜ M*W, is the set of all possible ordered pairs of form (m,w), where mÏµM and wÏµW 
 
 
ï‚§ Matching: A matching â€œSâ€ is a set of ordered pairs, each from M*W, with the property 
that each member of M and each member of W appears in at most one pair in S. 
 
Perfect matching 
ï‚· A perfect matching S1 is a matching with the property that each member of M and each 
member of W appears in exactly one pair in S1. 
ï‚· A perfect match is a way of pairing men with the women in such a way that everyone 
ends up married to somebody and nobody is married to more than one person. (i.e. 
neither singlehood nor polygamy). 
Instability: 
 
ï‚· Say there are 2 pairs (m,w) and (m1,w1) in S with property that 
ïƒ˜ m prefers w1 to w 
ïƒ˜ w1 prefers m to m1 
CHAPTER 1 
INTRODUCTION 
 
 
The pair (m,w1) is an instability with respect to S: (m,w1) does not belong to S. 
ï‚· Our goal is a set of marriages with no instabilities. A matching S is stable if: 
1. It is perfect. 
2. There is no instability with respect to S. 
 
ï‚· Example 1: 
We have a set of two men, {m, m1} and a set of two women {w, w1}. The preference lists 
are: 
m prefers w to w1 
m1 prefers w to w1 
w prefers m to m1 
w1 prefers m to m1 
ïƒ˜ There is a unique stable matching, consisting of pairs (m,w) and (m1,w1). 
ïƒ˜ (m1,w) and (m,w1) would not be a stable match, because the pair (m,w) would 
form an instability with respect to this matching. 
 
ï‚· Example 2: 
m prefers w to w1 
m1 prefers w1 to w 
w prefers m1 to m 
w1 prefers m to m1 
 
ïƒ˜ (m,w) and (m1,w1) is stable, because both men are happy as neither would leave 
their matched partners. 
ïƒ˜ (m1,w) and (m,w1) is stable as both women are happy. 
ïƒ˜ So its possible for an instance to have more than one stable matching. 
 
1.3 
Designing the Algorithm: 
Basic steps: 
1. Initially, everyone is unmarried. 
ïƒ˜ If an unmarried man m chooses woman w who ranks highest on his preference list 
and proposes her. 
ïƒ˜ A mam m1 whom w prefers, w may or may not receive a proposal from m1. 
 
CHAPTER 1 
INTRODUCTION 
 
 
ïƒ˜ So w prefers to go to an intermediate state (m,w). 
 
2. Suppose we are now at a state in which some men and women are engaged and some 
of them are not engaged. 
ïƒ˜ An arbitrary free man m chooses the highest ranks woman w and proposes her. 
ïƒ˜ If w is free, then m and w become engaged. 
ïƒ˜ Otherwise, w is already engaged to some other man m1 i.e. she determines which 
of m or m1 ranks higher on her preference list. 
3. Finally algorithm will terminate when no one is free. 
 
 
Algorithm 
Initially all mÏµM and wÏµW are free. 
While there is a man m who is free and hasnâ€Ÿt proposed to every woman 
Choose such a man m 
Let w be the highest ranked woman in mâ€Ÿs preference list to whom m has not yet 
proposed. 
If w is free then 
(m,w) become engaged. 
else 
w is currently engaged to m1 
 
ifw prefers m1 to m then 
m remains free 
Else 
w prefers m to m1 
(m,w) become engaged. 
m1 becomes free. 
End if 
End if 
End while 
Return the set S of engaged pair. 
 
 
 
CHAPTER 1 
INTRODUCTION 
 
 
1.4 
Analyzing the Algorithm 
1. w remains engaged from the point at which she receives her first proposal, 
and a sequence of partners to which she is engaged gets better and better(in 
terms of her preference list). 
 
2. The sequence of women to whom m proposes gets worse (in terms of his 
preference list). 
 
 
 
PROOF: 
3. The G-S algorithm terminates after at most n2 iterations of the while loop. 
 
ï‚· Let P(t) denote the set of pairs(m,w) such that m has proposed to w by the end 
of iteration t . 
ï‚· For all t,the size of P(t+1) is strictly greater than the size of P(t). 
ï‚· There are only n2 possible pairs of men and women in total. So the value of 
P(.) can increase at most n2 times over the course of the algorithm. 
ï‚· So it follows that there can be at most n2iterations. 
 
ï‚· Two things are important to note : 
1.  There are executions of the algorithm that can involve close to n2 
iterations. 
2. There are many quantities that would not have worked well as a progress 
measure for algorithm,since they need not strictly increase in each iteration. 
ï‚· Example: Number of free individuals could remain constant from one 
iteration to the next, as could the number of engaged pairs. So these quantities 
could not be used directly in giving an upper bound on maximum possible 
number of iterations. 
 
 
 
 
 
 
 
4. If m is free at some point in the execution of the algorithm, then there is a woman 
to whom he has not yet proposed. 
 
CHAPTER 1 
INTRODUCTION 
 
 
PROOF 
 
ï‚· Suppose there comes a point when m is free but has already proposed to every 
woman. 
ï‚· Then by 1, each of â€nâ€Ÿ women is engaged at this point in time. 
ï‚· Since the set of engage pairs forms a matching, there must also be â€nâ€Ÿ engaged men 
at this point in time. 
ï‚· But there are only â€nâ€Ÿ men total,and m in not engaged,so this is a contradiction. 
 
 
5. The set S returned at termination is a perfect matching. 
PROOF 
ï‚· A set of engaged pairs always forms a matching. 
ï‚· Let us suppose that the algorithm terminates with a free man m. 
ï‚· At termination, it must be the case that m had already proposed to every woman, for 
otherwise the while loop would not have exited. 
ï‚· But this contradicts (4) which says that there cannot be a free man who has proposed 
to every woman. 
 
6. Consider an execution of the G-S algorithm that returns a set of pairs S. the set S 
is a stable matching. 
 
PROOF 
 
ï‚· We have seen from (5) that S is a perfect matching. 
ï‚· To prove S is a stable matching, we will assume that there is an instability with 
respect to s and obtain a contradiction. 
ï‚· Instability would involve 2 pairs(m,w) and(m1,w1),in s with the properties that 
1. m prefers w1 to w,and 
2.w1 prefers m to m1 
ï‚· In the execution of algorithm that produced S,mâ€™s last proposal was,by definition to 
w. 
ï‚· Did m propose to w1 at some earlier point in the execution point in the execution? 
 
 
CHAPTER 1 
INTRODUCTION 
 
 
ïƒ˜ If he didnâ€Ÿt then w is higher on mâ€Ÿs preference list than w1,contradicting our 
assumption that m prefers w1 to w. 
ïƒ˜ If he did, then he was rejected by w1 in favour of some other man m11,whom 
w1 prefers to m. 
ïƒ˜ m1 is final partner to w1,so either m11=m1 or w1prefers final partner m1to 
m11,either way this contradicts our assumption that w1prefers m to m1. 
ïƒ˜ It follows that S is a stable matching. 
 
1.5 
Extensions 
 
All extensions yield the same matching: 
ï‚· Uniquely characterize the matching that is obtained and then show that all executions 
result in matching with this characterization. 
ï‚· We will show that each man ends up with the best possible partner. 
ï‚· A woman w is a valid partner of man m if there is a stable matching that contains the 
pair(m,w). 
ï‚· We will say that w is the best valid partner of m if w is a valid partner of m,and no 
woman whom m ranks higher than w is a valid partner of his. 
ï‚· We will use best(m) to denote the best valid partner of m. 
ï‚§ Now let S* denote the set of pairs{(m,best(m)):mÏµM}. 
ï‚· Each man ends up with the best possible partner if all men prefer different women. 
 
 
7. Every execution of the G-S algorithm results in the set S*: 
PROOF 
ï‚· Let us suppose by the way of contradiction, that some execution E of the G-S 
algorithm results in matching s in which some man is paired with woman who is not 
his best valid partners. 
ï‚· Since men propose in decreasing order of preference,this means that some man is 
rejected by a valid partner during the execution E of the algorithm. 
ï‚· So consider the first moment during the execution E in which some man, say m,is 
rejected bya valid partner w. 
 
 
 
CHAPTER 1 
INTRODUCTION 
 
 
ï‚· Since men propose in decreasing order of preference,E since this is the first time 
such a rejection has occurred,it must be that w is mâ€Ÿs best valid partner best(m). 
ï‚· The rejection of m by w may have happened either because 
1. m proposed and was turned down by w,because of existing engagement. 
2. w broke engagement to m in favour of better proposal. 
ï‚· Either way,w forms or continues an engagement with a man m1 whom she prefers to 
m. 
ï‚· Since w is a valid partner of m, there exists a stable matching S1 containing the 
pair(m,w). 
ï‚· Since the rejection of m by w was the first rejection of a man by a valid partner in the 
execution E,it must be that m1 has not been rejected by any valid partner at the point 
in E when he became engaged to w. 
ï‚· Since he proposed indecreasing order of preference, and since w1 is clearly a valid 
partner of m,it must be that m1 prefers to w to w1. 
ï‚· But we have already seen that w prefers m1 to m,for in execution E she rejected m in 
favour of m1. 
ï‚· Since(m1,w) does not belong to S1 ,it follows that (m1,w) is an instability in S1. 
ï‚· This contradicts our claim that S1 is stable and hence contradicts our initial 
assumption. 
 
8. In the stable matching S*, each woman is paired with her worst partner with her 
worst valid partner. 
 
PROOF: 
 
ï‚· Suppose there were a pair (m,w) in S* such that m is not the worst valid partner to w. 
ï‚· Then there is a stable matching S1 in which w is paired with a man m whom she likes 
less than m. 
ï‚· In S1, m is paired with a woman w1â‰ w; since w is the best valid partner of m and w1 
is a valid partner of m, we see that m prefers w to w1. 
ï‚· But from this it follows that (m,w) is an instability in S1, contradicting the claim that 
S1 is stable and hence contradicting our initial assumption. 
 
CHAPTER 1 
INTRODUCTION 
 
 
2. Five Representative Problems: 
1. Internal scheduling: 
 
Scheduling problem: 
ï‚· You have aresource say lecture room and many people request to use the resources for 
periods of time. 
ï‚·ï€ 
ï‚·ï€ 
ï€ 
ï€ 
ï‚·ï€ 
ï‚·ï€ 
ï€ 
ï€ 
ï€ 
ï€ 
ï€ 
ï€ 
ï€ 
ï€ 
ï€ 
ï€ 
ï€ 
ï€ 
ï€ 
ï€ 
ï€ 
ï€ 
ï€ 
ï€ 
ï€ 
ï‚·ï€ 
ï€ 
ï€ 
ï‚·ï€ 
for an earlier time interval than request â€jâ€Ÿ (fi<=Sj) or otherwise (fj<=Si). 
ï‚· A subset A of requests is compatible if all pairs of requests i,jÏµA. 
ï‚· The goal is to select a compatible subset of requests of maximum possible size. 
ï‚· This problem can be solved by algorithm that orders the set of requests according to 
certain heuristic and then â€œgreedilyâ€ process them, selecting as large compatible subset as 
it can. 
 
CHAPTER 1 
INTRODUCTION 
We will assume that the resource can be used at most one person at a time. 
A scheduler wants to accept a subset of these requests rejecting all others, Si that the 
accepted request donot overlap in time. 
The goal is to maximize the number of requests accepted. 
Example: 
A single compatible set of size 4, and this is the largest compatible set. 
There will be â€nâ€Ÿ requests labeled 1,2,â€¦.n, with each request ispacing start time Si and a 
finish time fi, such that Si<fi for all i. 
2 requests i and j are compatible if the requested intervals do not overlap. i.e. either â€iâ€Ÿ is 
 
 
 
2. Weighted Interval Scheduling: 
ï‚· In this, each request interval â€iâ€Ÿ has an associated value, or weight, â€œVi>0â€, we can 
picture this as amount of money we will make from the ith individual if we schedule his 
or her request. 
ï‚· Goal is to find a compatible subset of intervals of maximum total value. 
ï‚· If Vi=1 foe each â€iâ€Ÿ is simply the basic interval scheduling. 
ï‚· Appearance of arbitrary values changes the nature of maximization problem. 
ï‚· Example: If V1 exceeds the sum of all other â€œViâ€, then the optimal solution must include 
interval 1regardless of the configuration of full set of intervals. So any algorithm for this 
problem must be very sensitive to the values. 
ï‚· We employ â€œdynamic programmingâ€ technique that builds up the optimal value overall 
possible solution in a compact tabular way, that leads to efficient algorithm. 
 
3. Bipartite Matching : 
ï‚· We can express the concept of â€œstable matching problemâ€ more generally in terms of 
graphs, and in order to do this it is useful to define the notation of â€œbipartite graphâ€. 
ï‚· A graph G=(V,E) is bipartite, if its node set â€œVâ€ can be partitioned into sets X and Y in 
such a way that every edge has one end in â€Xâ€Ÿ and the other end in â€Yâ€Ÿ. 
ï‚· In case of bipartite graphs, the edges are pairs of nodes, so we say that matching in a 
graph G=(V,E) is a set of edges M proper subset of E with the property that each node 
appears in at most one edge of M. 
ï‚· M is a perfect matching if every node appears in exactly one edge of M 
ï‚· For stable matching, consider a bipartite graph G1 with a set â€Xâ€Ÿ of â€Ÿnâ€Ÿ men, a set Y of n 
women, and an edge from every node in â€Xâ€Ÿ to every node in â€Yâ€Ÿ. 
ï‚· In stable matching we added preferences, here we add a different source of complexity. 
ï‚· There is not necessarily an edge from every xÏµX to every yÏµY, so the set of possible 
matching has quite complicated structure. 
(OR)  
Only certain pairs of men and women are willing to be paired off and figure out how 
many people in a way that is consistent with this. 
 
 
CHAPTER 1 
INTRODUCTION 
 
 
ï‚· Example: X is the set of professors in the department. 
Y is the set of offered courses. 
(Xi,Yj) is an edge that indicates, professor Xibe of teaching courseYj. 
ï‚· A perfect matching consists of an assignment of each professor to a course that he/she 
can teach, in such a way that every course is covered. 
ï‚· To solve this problem we have an efficient algorithm called â€œAugmentationâ€. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Bipartite graph 
 
4. Independent Set: 
ï‚· Given a graph G=(V,E), we say a set of nodes S which is proper subset of V is 
independent if no two nodes in S are joined by an edge. 
ï‚· Independent set problem is, given a graph G, find an independent set that is as large as 
possible. 
 
 
 
 
 
 
 
 
 
 
 
CHAPTER 1 
INTRODUCTION 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Example: The maximum size of an independent set in the graph is 4. i.e. {1, 4, 5, 6} 
 
ï‚· Independent set problem encodes situation in which you have â€nâ€Ÿ friends and some pairs 
donâ€Ÿt get along. How large a group of friends can be invited to dinner if you donâ€Ÿt want 
any interpersonal tensions, L would be the largest independent set in the graph whose 
nodes are your friends, with an edge between each conflicting pair. 
ï‚· Interval scheduling and bipartite matching are special cases of independent set problem. 
ï‚· For interval scheduling, define a graph G= (V, E) in which the nodes are the intervals 
and there is an edge between each pair of them that overlap. 
ï‚· The independent sets in G are then just the compatible subsets of intervals. 
ï‚· Given a bipartite graph G1= (V1, E1), the objects being chosen are edges, and the 
conflicts arise between two edges that share an end. 
ï‚· So we define a graph G= (V, E) in which the node set V is equal to the edge set E1 of G1. 
ï‚· We define an edge between each pair of elements in V that corresponds to edges of G1 
with a common end. 
ï‚· We can now check that the independent sets of G are precisely the matchings of G1. 
 
5. Competitive Facility Location 
ï‚· Consider 2 large companies that operate cafÃ© franchise across the country and they are 
currently competing for market share in a geographical area. 
ï‚· First â€œCafÃ© Coffee Dayâ€ opens a franchise, then â€œCoffee Worldâ€ opens a franchise and so 
on. 
 
 
CHAPTER 1 
INTRODUCTION 
 
 
ï‚· Suppose they deal with zoning regulations that require no two franchises be located too 
close together, and each is trying to make its locations as convenient as possible. 
ï‚· The geographical region is divided into â€œnâ€ zones, labeled 1,2,3â€¦â€¦,n. Each zone â€œiâ€ has 
a value â€œbiâ€, which is the revenue obtained by either of the companies if it opens a 
franchise there. 
ï‚· 
 Finally certain pairs of zones (i, j) are adjacent, and local zoning laws prevent two 
adjacent zones from each containing a franchise, regardless of which company owns them. 
ï‚· We model these conflicts via a graph G= (V, E), whose â€œVâ€ is a set of zones, and (i, j) is 
an edge in E if the zones â€œiâ€ and â€œjâ€ are adjacent. 
ï‚· Zoning requirement says that the full set of franchises opened must form an independent 
set in G. 
 
 
 
 
ï‚· Our game consists of 2 players, P1 and P2, alternately selecting nodes in G, with P1 
moving first. 
ï‚· The set of all selected nodes must form an independent set in G. 
ï‚· P2 target bound is B=20. Then P2 has a winning strategy. 
ï‚· If B=25, then P2 does not have a winning strategy. 
ï‚· If there is a strategy for P2 so that no matter how P1 plays, P2 will be able to select a set 
of nodes with a total value of atleast B. We call this an instance of â€œcompetitive location 
problemâ€. 
 
***************** 
 
 
 
 
 
 
 
 
 
 
 
 
CHAPTER 1 
INTRODUCTION 
 
 
CHAPTER 2 
COMPUTATIONAL TRACTABILITY 
ï‚· Primary focus will be on the efficiency in running time. i.e. we want algorithms to run 
quickly. 
ï‚· Algorithms must be efficient in their use of other resources as well. i.e space used by an 
algorithm. 
 
Some initial attempts of defining Efficiency 
ï‚· Definition: An algorithm is efficient if, when implemented, it runs quickly on real input 
instances. 
ï‚· Disadvantages in definition: 
1. Omission of where we implement algorithm and howwell we implement algorithm. 
a. Even bad algorithm will run quickly when applied to small test cases on 
extremely fast processors. 
b. Even good algorithm runs slowly when they are coded sloppily. 
2. We donâ€Ÿt know the full range of input instances that will be encountered in practice. 
3. Does not consider how well or badly an algorithm may scale as problem sizes grow 
to unexpected levels. 
ï‚· Situation where 2 very different algorithms may perform comparably on input size 100; 
multiply the input size tenfold and one will still run quickly while other consumes a huge 
amount of time. 
ï‚· We need a concrete definition of efficiency that is 
ïƒ˜ Platform independent. 
ïƒ˜ Instance independent 
ïƒ˜ Predictive values w.r.t increasing input size. 
ï‚· Example: Stable matching problem. 
ïƒ˜ N ïƒ  Input size (total size of preference lists) 
ïƒ˜ nïƒ  Number of men and women. 
ïƒ˜ There are 2n preference lists, each of length â€œnâ€. 
ïƒ˜ N=2n2 (2 preference lists each of length n) 
 
 
CHAPTER 1 
INTRODUCTION 
 
 
ï‚· Worst-Case Running times & Brute-Force Search 
ï‚· In analysis of worst-case running time, we will look for a bound on largest possible 
running time the algorithm could have over all inputs of a given size â€œNâ€ & see 
how it scales with â€œNâ€. 
ï‚· Brute-Force search can tell us whether a running time bound is impressive or weak 
over search space. 
ï‚· Example: Stable matching problem. 
ï‚· When the size of a stable matching input instance is relatively small, the search 
spaceit defines is enormous (there are n! possible perfect matching between n men 
& n women), and we need to find a matching that is stable. 
ï‚· Brute force algorithm for this problem would plow through all perfect matchings 
by enumeration, checking each to see if it is stable. 
ï‚· In this we need to spend time proportional only to N in finding a stable matching 
from this stupendously large space of possibilities. 
 
ï‚· Proposed definition of Efficiency: An algorithm is efficient if it achieves qualitatively 
better worst-case performance at an analytical level, than brute-force search. 
ï‚· Note: It is very hard to express full range of instances that arise in practice. So how a 
random input should be generate, as same algorithm can perform very well on one class of 
random inputs & very poorly on another. 
 
Polynomial time as definition of Efficiency. 
 
ï‚· Search spaces for natural combination problems tend to grow exponentially in the size N 
of the input. 
ï‚· If input size increases by 1, the number of possibilities increases multiplicatively. 
ï‚· Algorithm for such a problem to have a better scaling property when input size increases 
by a constant factor (a factor of 2) the algorithm should only slow down by some constant 
factor â€câ€Ÿ. 
ï‚· Arithmetically we can formulate this scaling property as say: There are absolute constants 
c>0 & d>0 so that on every input instance of size â€œNâ€ its running time is bounded by cNd 
primitive computational steps. 
CHAPTER 1 
INTRODUCTION 
 
 
ï‚· If running time bound holds, for some c & d, then we say that the algorithm has a 
polynomial running time. 
ï‚· If input size increases from N to 2N, the bound of running time increases from cNd to 
c(2N)d = c.2d.Nd, which is a slow-down by a factor of 2d. Since d is a constant so is 2d. 
ï‚· Lower-degree polynomials exhibit better scaling behavior than higher-degree 
polynomials. 
 
Proposed definition of Efficiency: 
ï‚· An algorithm is efficient if it has a polynomial running time. 
ï‚· Problems for which polynomial-time algorithm exists almost invariably turnout to have 
algorithms with running time proportional to very moderate growing polynomials like n, 
n log n, n2 or n3. 
ï‚· Exponential functions 2n& n! grow fast that their values become astronomically large 
even for small values of n. 
ï‚· Algorithms that require an exponential number of operations are practical for solving 
only problems of very small sizes. 
 
Asymptotic order of growth 
1. Asymptotic upper bounds: 
ï‚· Let T(n) be a function- the worst case running time of a certain algorithm on an input 
size â€œnâ€. 
ï‚· Another function f(n), we say that T(n) is O(f(n)) if, for sufficiently large n, the function 
T(n) is bounded above by a constant multiple of f(n). 
(OR) 
T(n)= O(f(n)) 
ï‚· T(n) is O(f(n)) if there exist constant c > 0 and n0>= 0 so that for all n >= n0, we have 
T(n) <c.f(n). In this case, we will say that T is asymptotically upper bounded by f. 
ï‚· Example: Consider an algorithm whose running time is of the form T(n)=pn2+qn+r for 
positive constant p,q and r. 
ïƒ˜ We would like to claim that any such function is O(n2). 
ïƒ˜ T(n)=pn2+qn+r <= pn2+qn2+rn2 =(p+q+r)n2 for all n>=1 
ïƒ˜ This inequality is exactly is exactly what the definition of O(.) requires: 
T(n)<=c.n2 where c=p+q+r 
 
CHAPTER 1 
INTRODUCTION 
 
 
ïƒ˜ O(.) expresses only an upper bound. 
ïƒ˜ So we claim that the function T(n)=pn2+qn+r is O(n2), it is also correct to say 
that its O(n2). 
 
2. Asymptotic Lower bounds: 
ï‚· For arbitrarily large input sizes n, the function T(n) is at least a constant multiple 
of some specific f(n). 
ï‚· Then we say that T(n) is Î©(f(n)) if there exist constants c>0 and n0>0. So that for 
all n>=n0, we have T(n)>= c.f(n) 
ï‚· So we will refer to T in this case as being asymptotically lower bounded by f. 
ï‚· The constant â€œcâ€ must be fixed, independent of â€œnâ€. 
ï‚· Example: T(n)=pn2+qn+r, where p,q and rare positive constants. 
ï‚· We need to reduce the size of T(n) until it looks like a constant times n2. 
ï‚· T(n)=pn2+qn+r>=pn2 for all n>=0 
ï‚· So T(n)=pn2+qn+r is Î©(n), since T(n)>=pn2>=pn. 
 
3. Asymptotically Tight bound: 
ï‚· If a function T(n) is both O(f(n)) and Î©(f(n)), we say that T(n) is Î¸(f(n)). In this 
case, we say that f(n) is asymptotically tight bound for T(n). 
ï‚· So for example our analysis shows that T(n)= pn2+qn+r is Î¸(n2) 
ï‚· Asymptotically tight bound characterize the worst-case performance of an 
algorithm precisely up to constant factor. 
 
Properties of Asymptotic Growth rates 
 
1. Let f and g be 2 functions that ğ¥ğ¢ğ¦ğ’â†’âˆ 
 
(
ğ’‡(ğ’)) exists and is equal to some number 
ğ’ˆ(ğ’) 
c>0. Then f(n)=Î¸(g(n)) 
Proof: 
ï‚· We will use the fact that the limit exists and is positive to show that f(n)= O(g(n)) and 
f(n)= Î©(g(n)) as required by the definition of Î¸(.) 
Sincelimğ‘›â†’âˆ (ğ‘“(ğ‘›))   c >0 
ğ‘”(ğ‘›) 
 
 
 
 
CHAPTER 1 
INTRODUCTION 
ğ‘”(ğ‘›) 
1 
 
 
ï‚· It follows from the definition of limit that there is some n0 beyond which the ration is 
always between Â½ c and 2c. 
ï‚· Thus f(n) <= 2c.g(n) for all n>=n0, which implies that f(n)=O(g(n)) and f(n) >= Â½ c.g(n) 
for all n>= n0, which implies that f(n)= Î©(g(n)) 
ï‚· Hence f(n)=Î¸(g(n)). 
 
2. Transitivity:If a function f is asymptotically upper bounded by a function g and if g 
in turn is asymptotically upper bounded by a function h, then f is asymptotically 
upper bounded by h. 
 
2.1. 
If f=O(g) and g=O(h), then f=O(h) 
Proof: 
f(n)= O(g(n)), 
 
lim  
ğ‘›â†’âˆ (
ğ‘“(ğ‘›)) 
c1=0 or c1>0 for all n0>0 ïƒ  (1) 
ğ‘”(ğ‘›) 
 
 
g(n)= O(f(n)), 
 
lim 
 
ğ‘›â†’âˆ ( 
) c2=0 or c2>0 for all n0 >0 ïƒ  (2) 
ğ‘•(ğ‘›) 
 
 
Multiply (1) and (2) 
limn âˆ   
ğ‘“(ğ‘›) Ã— ğ‘”(ğ‘›) c1c2=0 or c1c2>0 
ïƒ  
ğ‘”(ğ‘›) 
ğ‘•(ğ‘›) 
 
lim  
ğ‘›â†’âˆ (
ğ‘“(ğ‘›))   or f(n)= c.h(n) 
ğ‘•(ğ‘›) 
Hence f(n) = O(h(n)) 
 
 
2.2. 
If f=Î©(g) and g= Î© (h), then f= Î© (h) 
Proof: 
f(n)= Î© (g(n)), 
 
lim  
ğ‘›â†’âˆ (
ğ‘“(ğ‘›)) 
c1=âˆ or c1>0 for all n0>0 ïƒ  (1) 
ğ‘”(ğ‘›) 
 
 
 
CHAPTER 1 
INTRODUCTION 
ğ‘”(ğ‘›) 
1 
ğ‘”(ğ‘›) 
1 
 
 
g(n)= Î© (f(n)), 
 
lim 
 
ğ‘›â†’âˆ ( 
) c2=âˆ or c2>0 for all n0 >0 ïƒ  (2) 
ğ‘•(ğ‘›) 
 
 
Multiply (1) and (2) 
limn âˆ   
ğ‘“(ğ‘›) Ã— ğ‘”(ğ‘›) c1c2=âˆ or c1c2>0 
ïƒ  
ğ‘”(ğ‘›) 
ğ‘•(ğ‘›) 
 
lim  
ğ‘›â†’âˆ (
ğ‘“(ğ‘›))   or f(n)= Î© (h(n)) 
ğ‘•(ğ‘›) 
 
 
2.3. 
If f=Î¸(g) and g= Î¸ (h), then f= Î¸ (h) 
Proof: 
f(n)= Î¸ (g(n)), 
 
lim  
ğ‘›â†’âˆ (
ğ‘“(ğ‘›)) 
c1>0 for all n0>0 ïƒ  (1) 
ğ‘”(ğ‘›) 
 
 
g(n)= Î¸ (f(n)), 
 
lim 
 
ğ‘›â†’âˆ ( 
) c2>0 for all n0 >0 ïƒ  (2) 
ğ‘•(ğ‘›) 
 
 
Multiply (1) and (2) 
limn âˆ   
ğ‘“(ğ‘›) Ã— ğ‘”(ğ‘›) 
c1c2>0 
ïƒ  
ğ‘”(ğ‘›) 
ğ‘•(ğ‘›) 
 
lim  
ğ‘›â†’âˆ (
ğ‘“(ğ‘›))   or f(n)= c.h(n) 
ğ‘•(ğ‘›) 
Hence f(n) = Î¸ (h(n)) 
 
 
2.4. 
Suppose that f and g are two functions such that for some other function h we 
have f=O(h) and g=O(h). Then f+g= O(h) 
Proof: 
 
ï‚· We are given that for some constants c and n0, we have f(n) <= c1.h(n) for all 
n>=n01 
ï‚· So consider any number n that is at least as large as both n0 and n01. 
 
CHAPTER 1 
INTRODUCTION 
 
 
ï‚· We have, 
f(n)+g(n) 
<= c.h(n) + c1.h(n) 
f(n)+g(n)<=(c+c1).h(n) for all n>= max(n0, n01) 
ï‚· Which is exactly what is required for showing that f+g=O(h). 
 
2.5. 
Let k be a fixed constant, and let f1, f2, f3,â€¦..,fk and h be functions such that 
fi=0(h) for all i. Then f1+f2+f3+â€¦â€¦+fk = O(h) 
ï‚· Consequence of (2.4) 
ï‚· We are analyzing an algorithm with two high level parts and it is easy to show 
that one of the two parts is slower than the other. 
ï‚· The running time of the whole algorithm is asymptotically comparable to the 
running time of the slow part. 
ï‚· Since overall running time is a sum of 2 functions, result on asymptotic bounds 
for sums of functions are directly relevant. 
 
2.6. 
Suppose that f and g are two functions such that g=O(f). Then f+g=Î¸(f). in 
other words, f is an asymptotically tight bound for the combined function f+g. 
Proof: 
g(n) = O(f(n) 
g(n)<= f(n) 
or 
f(n)>=g(n) ïƒ Î© 
f(n) + g(n) >= Î©(f(n)) for all n>=0 
i.e. f(n) + g(n) >=f(n) 
We have already shown that f+g=O(f) 
Hence if functions belongs to O(n) and Î©(n), then it has to belong to Î¸(n) 
 
2.7. 
Let f be a polynomial of degree k, in which the coefficient ak is positive. Then 
f= O(nk) 
Proof: 
f(n)= aknk + ak-1nk-1 +â€¦â€¦ +a1n + a0 
The highest order in polynomial that dominates as nïƒ âˆ 
Choose n0=1 and c=|ak| + |ak-1| + â€¦..+ |a1| + |a0| 
We need to show that f(n)=c.nk for all n>=1. 
We have for every n>=1, we replace coefficients with absolute value. 
 
 
CHAPTER 1 
INTRODUCTION 
 
 
f(n)<=|ak|nk + |ak-1|nk-1 +â€¦..+ |a1|n + |a0| 
Replace lower power of n with higher powers of nk. 
f(n)<= |ak|nk + |ak-1|nk +â€¦â€¦+ |a1|nk + |a0|nk 
f(n)<= (|ak| + |ak-1| +â€¦â€¦+ |a1| + |a0|)nk 
f(n)<=c.nk 
or 
f(n)=O(nk) 
 
2.8. 
For every b>1 and every x>0, we have ğ¥ğ¨ğ ğ’ƒ ğ’= O(nx) 
Proof: 
One can translate directly between logarithms of different bases using identity. 
logğ‘ 
 
logğ‘ 
ğ‘› =log ğ‘ ğ‘› 
log ğ‘ ğ‘ 
 ğ‘›=  
1 
log ğ‘ ğ‘ 
 
 
*logğ‘ ğ‘› 
logğ‘ ğ‘› = C. logğ‘ ğ‘› 
logğ‘ ğ‘› =Î¸(logğ‘ ğ‘›) 
Note: Base of the algorithm is not important when writing bounds using asymptotic 
notation. 
 
2.9. 
For every r>1 and every d>0, we have nd= O(rn) 
ï‚· For different bases r>s>1, it is never the case that rn=Î¸(sn) [unlike in log]. 
ï‚· This would require that for some constant c>0, we would have rn<=c.sn for all 
sufficiently large n. 
rn<=c.sn 
Divide by sn 
rn/sn<=c for all sufficiently large â€œnâ€ 
(or) 
(r/s)n<= c 
Since r>s, the expression (r/s)n tends to infinity with â€œnâ€, and so it cannot 
possibly remain bounded by a fixed constant c. 
 
 
 
 
CHAPTER 1 
INTRODUCTION 
 
 
A Survey of Common Running Times: 
1. Linear time: 
ï‚· An algorithm that runs in O(n), or linear, time has a very natural property: its running 
time is at most a constant factor times the size of the input. 
ï‚· Example 1: Computing the maximum 
ï‚· Numbers are provided as input in either list or array. 
ï‚· Each time we encounter a number ai, we check whether ai is larger than current 
maximum, and if so we update the estimate to ai. 
 
ï‚· Algorithm: 
max=a1 
fori=2 to n 
ifai>max then 
Set max=ai 
End if 
End for 
ï‚· f(n)=O(n) 
Example 2: Merging two sorted lists 
 
ï‚· Suppose we are given two lists of n numbers each, a1,a2,a3, â€¦â€¦ , an and b1, b2, 
b3,â€¦â€¦. , bn and each is arranged in ascending order 
ï‚· Merge them into a single list c1, c2, c3, â€¦â€¦ c2n that is also arranged in ascending 
order. 
 
ï‚· Algorithm: 
To merge sorted lists A=a1, a2, â€¦.., an and B= b1, b2,â€¦.. bn, 
Maintain a current pointer into each list, initialized to point to the front elements 
While both lists are non-empty 
Let ai and bj be the elements pointed to by the current pointer. 
Append the smaller of these two to the output list. 
Advance the current pointer in the list from which the smaller element was 
selected. 
End while 
Once one list is empty, append the remainder of the other list to the output. 
 
ï‚· Order of growth is O(n). 
 
 
 
CHAPTER 1 
INTRODUCTION 
Note: We are not considering 4th layer because we are not doing any processing here, we 
split and we go back. 
ï‚· Layer 1 has 8 elements, so as layer 2 and layer 3. 
8+8+8=24 
ï‚· 3 levels ïƒ  8 elements per level. 
ï‚· So total operations = 8*3= n* ? 
ï‚· 8â†”3 
ï‚· log2 8â†”3 
ï‚· log2 23â†”3 
ï‚· 3log2 2â†”3 
ï‚· 3*1â†”3 
 
 
 
 
2. O(n log n) time 
Mergesort: Algorithm that splits its inputs into 2 equal sized pieces, solve each piece 
recursively & combines the set in linear time. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
ï‚· Or log2 8â†”3 
ï‚· i.e. 8*log2 8 
ï‚· O(n log n) 
 
 
 
 
 
 
 
CHAPTER 1 
INTRODUCTION 
 
 
3. Quadratic time 
ï‚· Example: Suppose given n points in a plane, each specified by (x,y) coordinates, we 
would like to find the pair of points that are closest together. 
Solution: The number of pairs of points can be calculated by using formula ğ‘›(ğ‘›âˆ’1) 
2 
The distance between point (xi,yi) and (xj,yj) can be computed by the formula, 
d= âˆš(ğ‘¥ğ‘– âˆ’ ğ‘¥ğ‘—)2 + (ğ‘¦ğ‘– âˆ’ ğ‘¦ğ‘—)2 in constant time. 
So total number of comparison ğ‘›(ğ‘›âˆ’1)= O(n2) 
2 
 
 
ï‚· Algorithm: 
For each input point (xi,yi) 
For each output point (xj,yj) 
Compute distance d= âˆš(ğ‘¥ğ‘– âˆ’ ğ‘¥ğ‘—)2 + (ğ‘¦ğ‘– âˆ’ ğ‘¦ğ‘—)2 
If d is less than the current min, update min to d. 
End For 
End For 
 
 
4. Cubic Time 
ï‚· Sets of nested loop often leads to algorithm that run in O(n3) time. 
ï‚· Example: Given sets S1, S2,â€¦.., Sn, each of which is a subset of {1,2,3,â€¦..,n} and we 
would like to know whether some pair of these sets is disjoint. 
 
ï‚·  Algorithm: 
For each set Si 
For each set Sj 
For each element P of Si 
Determine whether P also belongs to Sj 
End For 
If no element of Si belongs to Sj then 
Report that Si and Sj are disjoint. 
 
CHAPTER 1 
INTRODUCTION 
 
 
End if 
End For 
End For 
 
ï‚· Each set has maximum size O(n), so the innermost loop takes time O(n), looping over set 
Sj involves O(n) iterations and looping over set Si involves O(n). 
ï‚· Multiplying these three factors of â€œnâ€ together we get the running time of O(n3) 
 
 
5. O(nk) time 
ï‚· Example: Finding independent sets in a graph. 
ï‚· For some fixed constant â€Kâ€Ÿ, we would like to know if given n-node input graph â€Gâ€Ÿ 
has an independent set of size â€Kâ€Ÿ 
 
Algorithm: 
For each subset S of K nodes 
Check whether S constitutes an independent set 
If S is an independent set then 
Stop and declare success 
End if 
End for 
If no K-node independent set was found then 
Declare failure 
End if 
 
Running time: 
ï‚· For running time consider â€œTotal number of K-element subsets in n-element set. 
ï‚· (ğ‘›)   ğ‘›(ğ‘›âˆ’1)(ğ‘›âˆ’2)â€¦â€¦.(ğ‘›âˆ’ğ‘˜+1) â‰¤ 
ğ‘›ğ‘˜ 
ğ‘˜     ğ‘˜(ğ‘˜âˆ’1)(ğ‘˜âˆ’2)(ğ‘˜âˆ’3)â€¦..(2)(1) 
ğ‘˜! 
ï‚· Since we treat â€Kâ€Ÿ as constant, this quantity is O(nk). So outer loop will run for 
O(nk) iterations as it tries all K-node subsets of the â€nâ€Ÿ nodes of the graph. 
ï‚· Inside loop we need to test a given set â€Sâ€Ÿ of â€Kâ€Ÿ nodes constitutes an independent 
set. 
ï‚· Definition of independent set says that we need to check for each pair of nodes, 
whether there is an edge or not. 
It is same as searching over pair of points which took O(k2) time. 
 
CHAPTER 1 
INTRODUCTION 
 
 
Therefore, the total running time is O(k2nk). 
Since we are treating â€Kâ€Ÿ as a constant we write the running time as O(nk). 
 
6. Beyond polynomial time 
ï‚· Two kinds of bounds that come up very frequently 2n and n! 
ï‚· Example: Given a graph we want to find an independent set of maximum size. 
ï‚· Algorithm 
For each subset S of nodes 
Check whether S constitutes an independent set 
If S is larger independent set than the largest seen so far then, 
Record the size of â€Sâ€Ÿ as the current maximum 
End if 
End for 
ï‚· We iterate over all subsets of graph. So the total number of subsets of n-element 
set is 2n. ïƒ  (1) 
ï‚· Outer loop executes 2n iterations as it tries all subsets. 
ï‚· Inside loop checks all pairs from set â€Sâ€Ÿ that can be as large as â€nâ€Ÿ nodes will take 
O(n2) time. ïƒ  (2) 
ï‚· Multiply (1) and (2) 
O(n22n) is the total running time for this algorithm. 
 
ïƒ° n! grows rapidly than 2n 
ï‚· n! is the number of ways to match up â€nâ€Ÿ items with â€nâ€Ÿ other items. 
ï‚· Example : Number of possible perfect matchings of â€nâ€Ÿ men and â€nâ€Ÿ women in an 
instance of the stable matching problem. 
ï‚· First man will always have â€nâ€Ÿ choices, once he selects a women from other set, 
the second man will select from (n-1) women left. 
ï‚· Multiplying all these choices we get 
n(n-1)(n-2)â€¦â€¦.(2)(1) = n! 
 
 
 
 
 
CHAPTER 1 
INTRODUCTION 
ï‚· To search 1 ïƒ Firsti will choose 5, then 2 and finally 1, i.e. i will do 3 searches to 
get an element. 
The total number of elements is 9, searches 3. 
9ïƒ³3 need to establish a relationship between 9 & 3. 
log2 9 = 3 
log2 9 â‰ˆ log2 8 
log2 8 = 3 
log2 23= 3 
 
 
7. Sub linear time 
ï‚· Cases where running times are asymptotically smaller than linear. 
ï‚· Goal is to minimize the amount of query that must be done. 
ï‚· Example: Binary Search 
 
3log2 2 = 3 
3=3 
Or log2 9 = 3 
i.e. 9 is the number of elements so replace 9 by n. 
O(log2 ğ‘›) is the worst case running time for binary search. 
 
CHAPTER 1 
INTRODUCTION 
 
 
ï‚· When 22 is being searched 
n= 9 No. of searches =4 
4â†”9 
1+3â†”9 
 
1+log2 9 = 1+ log2 23 
 
 
 
 
 
 
 
 
 
 
 
 
1+log2 9 = 1+ 3log2 2 
1+log2 9 = 1+ 3 
1+log2 9 = 4 
Or 1 + log2 ğ‘› 
i.e. O(log2 ğ‘›) is the worst case running time. 
ï‚· For successful search it is either log n or less than that. 
************************* 
CHAPTER 1 
INTRODUCTION 
